\section{Discussion}\label{sec:disc}

\hspace{13pt}  Support for the proposed hypotheses is partial. In the case of sex/gender, I find that datasets for use in biology are statistically and practically significantly more likely to refer to columns measuring sex/gender effects as ``Sex,'' but further that the entries within these columns are also statistically significant. However, I argue that, in regard to Hypothesis $2(a)$, this finding is not practically significant. The fact that the distributions have any number of entries in common should be striking, let alone the similarity in distribution shown in Figure \ref{fig:common_entries}. For these reasons, I argue that this claims-making on the part of biologists, as supported by Hypothesis $1(a)$, is purely symbolic---sex and gender are viewed as interchangeable terms for the same phenomena, (more exactly, for sex,) and biologists are more likely to use the term ``Sex'' in order to suggest a more purely biological mechanism for effects observed in the data.

However, in regard to race/ethnicity, my findings are limited. While data for use in biological contexts was likely to refer to columns measuring race/ethnicity effects as ``Race'' than other data, this difference is not statistically significant. Further, the sample size for common entries measuring race/ethnicity effects did not meet the assumptions of the $\chi^2$ goodness-of-fit test, so Hypothesis $2(b)$ remains untested. Some limitations to the methods used in this study contributing to this result are described below.

Principally, every decision-making procedure integrated into the data-scraping program is associated with some measure of error. For one, the inference on a packages purpose/intent is coarse and approximate. Based on the presence of keywords such as ``genome'' or ``epidem'' in package names and descriptions, the procedure infers that a package is likely to be used in biological contexts. I argue, though, that misclassification error in this instance would result in smaller effect sizes, and thus decrease the likelihood of statistical significance. In this way, the inference procedure produces results that are overly conservative. Moving on, this procedure rests on the assumption that, if CRAN users are measuring sex/gender or race/ethnicity effects in their data, they would name their columns as such. Again, I argue that the violation of this assumption does not result in the exacerbation of observed effects---if these columns are named imprecisely, then the entries within them should be similarly imprecisely coded. Altogether, then, I argue that the errors associated with each of these procedures results in, in reference to Hypothesis 1, overly conservative estimates of effects. In regard to Hypothesis 2, the entry ``cleaning'' procedure consolidates values that are inferred to be equivalent. Most of this consolidation is trivial (e.g. ``male'' to ``Male.'') In some cases, though, these decisions require more nuance; most notably, in columns named ``Sex'' and ``Gender'' with binary entries ``0'' or ``1,'' I recode these entries as ``Female'' and ``Male,'' respectively, reflecting the common practice of encoding males as ``successes'' in binary variables. Though I manually checked that this procedure did not misclassify any entries in my initial sample (either switching ``Male'' and ``Female'' or subsituting some other value entirely,) this manual validation procedure was not automated (and thus not immediately scrutinizable or reproducible.) In the case of tests relevant to Hypothesis 2, then, excessive error results in the assumptions of the $\chi^2$ goodness-of-fit test to be violated, as insufficiently large proportions of the sample are left ``unbinned,'' and thus assessment of significance to be inappropriate (as was the case with testing Hypothesis $2(b)$.)

In addition to quantifying and scrutinizing the error associated with the procedures outlined above, future work utilizing similar methodology can take some other steps in order to examine these claims more thoroughly. Most notably, a larger sample would allow for rigorous testing of statistical significance of Hypothesis $2(a)$. In general, too, consideration of social divisions beyond the four examined in this paper would allow for more nuanced understanding of the ways that these categories are argued in data in general. At the same time, these four social categories were chosen, among other reasons, for their prominence as encoded variables---for variables that are less likely to be measured and encoded in data, inversely proportionally larger samples will be necessary for valid inference. Further, the binning of entries that did not match either of the two most common entries in these columns ignores a significant portion of the structure of this data; the development of methods to examine these ``unmatched'' entries, and the relationships between them, in a more nuanced way could provide significant insight into the claims-making evident in these datasets. Lastly, future work should incorporate time into analyses in order to better understand how the way that these conceptualizations are encoded has changed over time. 

Altogether, I have attempted to show that datasets are sites of argumentation, claims-making, and boundary work that reflect social conceptualizations held by those constructing the data.
